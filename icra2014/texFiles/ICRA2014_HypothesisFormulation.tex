\section{Supervised Learning and Policy Gradient}
\label{sec:hyp_form}
% %Note that even for a single object ($K = 1$) determining the optimal cost in (\ref{eq:joint_optimization}) is intractable.
% %The complexity is a lot more manageable when optimizing with respect to a single object.
% %Reasons:
% %\begin{itemize}
% %	\item After every movement new objects might be discovered, which will require modifying the whole joint optimization
% %	\item Complexity is a lot more manageable when optimizing with respect to a single object; Even for a single object determining the optimal cost is intractable. 
% %\end{itemize}
% 
% The active object detection problem in Section \ref{sec:problem} is intractable because the spaces of the sensor pose $SE(3)$ and target orientations $SO(2)$ are continuous. Even if they are discretized, the joint optimization over the states of all $K$ objects is quite complex. It involves a trade-off between scene exploration and actions improving the confidence of the state estimates for different objects. Instead of solving the joint optimization problem over all objects on the table we break it up into $K$ independent sequential problems. To simplify notation we drop the object subscript $k$ because the problems are equivalent and minimize the following cost:
% \begin{equation}
% \label{eq:sequential_optimization}
% \mathbb{E}_{\overset{\mathcal{Q}_0,\ldots,\mathcal{Q}_{\tau}}{(y,r)}} \biggl\{ \sum_{t=0}^{\tau-1} g(x_t,u_t) + \Lambda_{y,r}^{R(\mathcal{Q}_{\tau})} \biggr\}.
% \end{equation}
% 
% 
% At a high-level planning stage we assume that we can work with a fully actuated model and let the possible actions be $W = \{w_1, \ldots, w_G\}$, each associated with a viewpoint:
% \[
% f(v_i,w_j) = v_j, \text{ for } v_i, v_j \in V(\rho) \text{ and } w_j \in W
% \]
% A common technique to abstract the dynamics of the mobile sensor is to use low level controllers and fast internal feedback loops \cite{Khatib86_internalloop, Cortes04_coverage_control}. Finally, as mentioned in Section \ref{sec:rel_work}, the space of object orientations is discretized sparsely as $\Theta = \{r_1,\ldots, r_N\} \subset SO(2)$. Thus, the hidden variables in the single object optimization (\ref{eq:sequential_optimization}) are the object class $y \in B_2$ and the object orientation $r \in \Theta$. The sensor needs to decide between $M = L_2 N + 1$ hypotheses:
% \begin{align*}
% H_0:&\text{ the object does not belong to $B_2$,}\\
% H_i:&\text{ the object is of class $C_{mod(i,L_2)} \in B_2$ with}\\
% &\text{ orientation $r_{(i-C)/L_2} \in R$ for }i = 1,\ldots,L_2 N
% \end{align*}
% To simplify notation we let:
% \[
% \Lambda_{ji} := \Lambda_{H_j}^{H_i} := \Lambda_{C_{mod(j,L_2)}, r_{(j-C)/L_2}}^{C_{mod(i,L_2)}, r_{(i-C)/L_2}}.
% \]
% The decision rule $R(\cdot)$ can be replaced by a set of decision actions $A = \{a_0, \ldots, a_{M-1}\}$ such that if the sensor takes action $a_i$, it retires and decides on hypothesis $H_i$. In other words, we enlarge the action space to $W \cup A$.
% 
% Given a hypothesis $H_i$ and a view-point $v \in V(\rho)$, the viewsphere can be rotated with the orientation of $H_i$. Several templates $\{\mathcal{P}_1,\ldots,\mathcal{P}_l\}_i$ from the vocabulary tree may correspond with this view of $H_i$. If any of the corresponding templates is returned as a top match, a score of $1$ is given to $H_i$; otherwise the score for $H_i$ is zero. Given a query point cloud $\mathcal{Q}$ extracted from $v$, the vocabulary tree assigns:
% \begin{align*}
% Z^0 &= VT(\mathcal{Q}, v, H_0) = \begin{cases} 
% 									0 & \text{, top match is in } B_2\\
% 									1 & \text{, top match is not in } B_2
% 								\end{cases}\\
% Z^i &= VT(\mathcal{Q}, v, H_i) = \begin{cases} 
% 									0 & \text{, top match is not in } \{\mathcal{P}\}_i\\
% 									1 & \text{, top match is in } \{\mathcal{P}\}_i
% 								\end{cases}
% \end{align*}

%Then, the closest template $\mathcal{P}$ from the training database $\mathcal{D}$ is associated with $H_i$ and $v$. Suppose that the template's descriptor is $d_{i,v}$. Then, the vocabulary tree can be used to determine a score associated with $H_i$ for a point cloud $\mathcal{Q}$ extracted from $v$ as follows:
%\begin{align*}
%Z^i = VT(\mathcal{Q}, v, H_i) = s(d_{i,v},q), \qquad &i = 0, \ldots, M-1,\\
%&v \in V(\rho)
%\end{align*}
%For every viewpoint $V$ each model $\mathcal{M}$ has a set of pose hypotheses associated with that specific view point. The matching score of these hypotheses are returned as the detection score $Z_t$.

%During the active hypothesis testing phase the set of detection scores $Z_t$ at time {\it t} corresponding to each view point $V$ is returned by a query function when a point cloud $Q_t$ is given as input.
%\begin{align*}
%Z_t = s(Q_t,V)
%\end{align*}
%Since every class in our database is associated with one specific object model $\mathcal{M}$, we use the words, class and models interchangeably. 
