\section{Problem Formulation}
\label{sec:problem}
Consider a set of rigid objects piled-up on any surface denoted $T_1,...,T_n \in \mathcal{T}$, where n is the number of objects. This set of rigid objects can be represented by a graph $\mathcal{G}$ where each vertex $g_i \in \mathcal{G}$ corresponds to a node with unique appearance. Here appearance can be defined by a number different attributes combined into a feature vector.

Every object in our set $\mathcal{T}$ can be represented by a rigid clique in this graph $\mathcal{G}$ comprising of one or more interconnected nodes $g_i$. Edges between rigid cliques are dynamic edges which are removed when two rigid objects are separated. Hence each rigid object corresponds to a set of nodes $\{g_{i1},..g_{in}\}$ in a clique $\mathcal{C}_i$. A collection of such cliques with 

with edges between them represents our scene graph denoted by $\mathcal{G}$. We cast the clutter segmentation problem as a graph separation problem where our objective is to pick a set of minimal actions to remove the dynamic edges from the graph and separate the graph into a set of minimal cliques each of which represent a rigid body.

In the initialization phase the dynamic edges between cliques represent the rigid objects that are either touching each other or entagled with each other in a pile of objects.

Given an initial scene graph $\mathcal{G}$ our objective is to pick the most optimal sequence of actions from a discrete set of actions $\{a_1,....a_m\} \in \mathcal{A}$ which optimizes the objective of graph separation and reduces the number of actions required to be executed. As the problem of trying to identify the set of actions that optimizes

the graph separation objective for the set of all possible graphs is intractable (given all possible set of graphs that one might encouter in natural scenes). To circumvent this problem we try to learn a mapping from actions $\mathcal{A}$ to features $\mathcal{F}$. These features $\mathcal{F}$ represent the current state of the environment. Instead of mapping actions to graphs we constraint the problem by trying

to learn a mapping between actions $\mathcal{A}$ and features $\mathcal{F}$. The quality of the features can be evaluated by the reward observed after executing each action. This learning problem can be cast as a supervised learning problem to classify the features based on action labels. Our learning approach discussed in detail Section \ref{sec:hyp_test}. Since we need to label features, we use Learning from 

Demonstration (LfD) to execute actions and label them with user specified rewards. We use a Max Entropy learner as the dimensionalty of the feature vector we use is much larger compared to the number of samples we collect.

As the dimensionalty of the feature space is incredibly high, we may still not be able to capture the entire variance in the feature space. To account for this variation, we let the learner adapt online to the features. Hence we utilize an online policy gradient styled approach where we optimized parametrized actions with respect to an expected reward by gradient descent. The details of the approach are dicussed in Section \ref{sec:hyp_test}.




% Let the table surface be represented by a bounded set $\mathcal{T} \subset \mathbb{R}^2$. Let $B_0$ be the possibly infinite set of all object classes that may appear on the table. We assume that each object class has a single model associated with it and use the words model and class interchangeably. Instances are drawn from $B_0$ at random and are placed uniformly on the table surface. For simplicity we assume that the objects have a random yaw and no pitch or roll so that their poses are in $SE(2)$. The extension of our framework to the $SE(3)$ case is immediate.
% 
% Consider a mobile depth sensor, whose position and orientation at time $t$ are $x_t = (x_t^p,x_t^r) \in SE(3)$. Let $\Omega$ represent the state of the \textit{static} environment, which includes factors such as scene geometry, lighting, occlusion, etc. At time $t$, the depth sensor can obtain a point cloud $\mathcal{Q}_t \subset \mathbb{R}^3$ from the scene which is visible from $x_t$ according to $\mathcal{Q}_t = \phi(x_t,\Omega)$. The first task of the sensor is to split $\mathcal{Q}_t$ into separate surfaces (\textit{segmentation}) and associate them with either new or previously observed objects (\textit{data association}). These procedures are not the focus of our paper but we mention briefly how we perform them in Subsection \ref{subsec:seg_data_ass}. We assume that they estimate the object positions accurately. 
% 
% The sensor has access to a database of size $L_1 < \infty$ of object models $B_1 \subset B_0$ and a subset $B_2 = \{C_1,\ldots,C_{L_2}\} \subseteq B_1$ of them are designated as objects of interest. The task of the sensor is to detect all objects from $B_2$, which are present on the table and to estimate their pose as \textit{quickly} as possible. Note that the detection is both against known objects from $B_1$ and unknown background from $B_0 \setminus B_1$.
% 
% We are interested in choosing a sequence of view-points for the mobile sensor, which has an optimal trade-off between energy used to move and number of incorrect classifications. Doing this with respect to all objects simultaneously results in a complex joint optimization problem. Instead, we treat the objects independently and process them sequentially, which simplifies the task to choosing a sequence of sensor poses with respect to a single object.
% 
% Further, we restrict the motion of the sensor to a sphere $S^2(\rho)$ of radius $\rho$, centered at the location of the object. The sensor's orientation is fixed so that it points at the centroid of the object. We denote this space of sensor poses by $V(\rho)$ and refer to it as a \textit{viewsphere}. A sensor pose $x \in V(\rho)$ is called a \textit{viewpoint}. As a result, we only need to plan for a sequence of viewpoints. At a high-level planning stage we assume that we can work with a fully actuated model of the sensor dynamics so that for any two poses $x^1, x^2 \in V(\rho)$, there exists a control $u^{1,2} \in U$, which takes the sensor from $x^1$ to $x^2$. At time $t$ the sensor can either move and make one more observation or decide on the class and orientation of the unknown object and retire.
% 
% As mentioned in Section \ref{sec:rel_work}, the space of object orientations is discretized sparsely as $\Theta = \{r_1,\ldots, r_N\} \subset SO(2)$. Thus, the hidden variables in the single object optimization are the object class $Y \in B_2$ and the object orientation $R \in \Theta$. The sensor needs to decide between $M = L_2 N + 1$ hypotheses:
% \begin{align*}
% H_0:&\text{ the object does not belong to $B_2$,}\\
% H_i:&\text{ the object is of class $\textit{cl}(H_i) := C_{mod(i,L_2)} \in B_2$}\\
% &\text{ with orientation $\textit{or}(H_i):= r_{(i-\textit{cl}(H_i))/L_2} \in \Theta$ for}\\
% &\; i = 1,\ldots,L_2 N
% \end{align*}
% In order to measure how well the task has been carried out we introduce the following costs:
% \begin{align*}
% \Lambda_{ij}&= \text{ cost for deciding on $H_i$, when $H_j$ is correct}\\
% &= \begin{cases}
%   		d(\textit{or}(H_i), \textit{or}(H_j)) & \textit{cl}(H_i) = \textit{cl}(H_j) \in B_2\\
%   		\Lambda_{fn} & \textit{cl}(H_i) \neq \textit{cl}(H_j) \in B_2\\
%   		\Lambda_{fp} & \textit{cl}(H_i) \in B_2, \textit{cl}(H_j) \in B_1 \setminus B_2\\
%   		0 & \text{if } i = j
%    \end{cases}\\
% c(x&^1,x^2) = d_{SE(3)}(x^1, x^2) + c_0 = \text{ cost of moving from $x^1$}\\
% & \qquad \text{to $x^2$ on the sphere and taking another observation}
% \end{align*}
% where $d_{SE(3)}(\cdot,\cdot)$ is a metric on $SE(3)$, $c_0 > 0$ is a fixed measurement cost, $\Lambda_{fp}$ and $\Lambda_{fn}$ are costs for making false positive and false negative mistakes respectively, and $d(\cdot,\cdot)$ is a cost for an incorrect orientation estimate, when the class is correct.
% 
% \begin{problem}[Active Object Detection]
% \label{prob:active_obj_detect}
% Given an object with random class $Y \in B_0$ and orientation $R \in SO(2)$ on the table $\mathcal{T} \ltimes SO(2)$, let $j^*(Y,R)$ denote the index of the hypothesis with the same class and closest orientation. The objective of the mobile sensor is to find a stopping time $\tau$, a sequence of viewpoints $x_0, \ldots, x_{\tau-1}$, and a decision rule $\delta(\mathcal{Q}_{1:\tau}) \in \{0,\ldots,M-1\}$ which minimize the cost:
% 
% \begin{align}
% \label{eq:sequential_optimization}
% \mathbb{E}_{\mathcal{Q}_{1:\tau},Y,R} \biggl\{ \sum_{t=0}^{\tau-1} c(x_t,x_{t-1}) + \Lambda_{\delta(\mathcal{Q}_{1:\tau}), j^*(Y,R)} \biggr\}.
% \end{align}
% \end{problem}
% 
% \begin{remark}
% The first term in (\ref{eq:sequential_optimization}) captures the energy spent moving, while the second term is a weighted probability of making a mistake. To see this suppose that all decision costs are equal, i.e. $D := d(\textit{or}(H_i), \textit{or}(H_j)) =\Lambda_{fn} = \Lambda_{fp}, \forall i,j$. Then:
% \begin{align*}
% \mathbb{E}_{\overset{\mathcal{Q}_{1:\tau},}{Y,R}}\biggl\{ \Lambda_{\delta(\mathcal{Q}_{1:\tau}), j^*(Y,R)} \biggr\} & = \mathbb{E}_{\overset{\mathcal{Q}_{1:\tau},}{Y,R}}\biggl\{ D \indicator_{\delta(\mathcal{Q}_{1:\tau}) \neq j^*(Y,R)}\biggr\}\\
% &= D \mathbb{P}( \delta \neq j^*),
% \end{align*}
% which is the probability that decision $\delta(\mathcal{Q}_{1:\tau})$ is incorrect. 
% \end{remark}
% 
% Our approach to solving the active object detection problem consists of two stages. First, we use a vocabulary tree to perform static detection in 3D. Since the detection scores are affected by sensor noise and occlusions we don't use them directly. Instead, we use a probabilistic framework to maintain the hypotheses about the detection outcome. In the second stage, we use non-myopic planing to select better viewpoints for the static detector and to update the probabilities of the hypotheses.





%\subsection{Object Detection}
%Our approach to object recognition involves extracting a set of templates $D = \{\mathcal{P}_1,\ldots,\mathcal{P}_G$ from the object model $\mathcal{M} \in B$. A template is extracted for each possible yaw $\theta \in R$ of the object of interest $\mathcal{M}$, by defining a set of viewpoints $V = \{v_1,\ldots,v_G\} \in S^2(\rho) \ltimes SO(3)$, which lie on a sphere of radius $\rho$ centered at the object. %where $\ltimes$ denotes a semidirect product


%Suppose that there is a finite number of three dimensional object classes $B = \{C_1,\ldots,C_L\}$ in the world. Object instances are drawn at random from $B$ and are placed uniformly on the table $\mathcal{T}$ with random yaw and no pitch or roll. We make the simplifying assumption that the object poses are in $SE(2)$ but the theoretical extension of our framework to the $SE(3)$ case is immediate given access to the model $\mathcal{M} \in B$ of one object of interest

