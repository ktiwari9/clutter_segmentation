\section{Learning From Demonstration}
\label{sec:lfd}
In our approach to clutter segmentation, since the learner/robot is not capable of exploring the entire space of all possible adjacency matrices for graph separation, we constraint the problem by exploring in the feature space rather than the space of all possible graphs. To accomplish this we define a set of features over the scene graph and execute actions on them 

and observe rewards. Each action is demonstrated by an expert who also labels the actions as good and bad based on the observed reward. So once an expert demonstrates and action, the action is executed on a randomly selected scene graph and the reward is observed. After observing a sequence of such rewards and the corresponding features computed for that specific scene graph,

we feed the features and their corresponding labels (1/0) into a Max Entropy learner to learn the weights for each action. These weights are then used in the online phase to select appropriate actions for a set of observed features. In the online phase the weights are also updated using a policy gradient approach discussed in Section \ref{sec:hyp_test}.

\subsection{Feature Selection}
To learn the utility of actions for a given scene graph $\mathcal{G}$, we define a set of features $\mathcal{F}$ over the scene graph. The features we use are color histograms, textons, grasp templates (\cite{Javidi12_Journal} TODO: Cite Alex here), push-shape templates and entropy maps. The computation of these features are discussed in detail in the following subsections

\subsubsection{Color Histogram}
To compute a color histogram over a given image patch we convert the patch from rgb to hsv color space and quantize the hue space into 3 bins and saturation space into 6 bins. The number of bins was selected heuristically to satisfy computational efficiency and sparsity constraints. So for each hue bin there are 6 corresponding saturation bins. Hence we quantize the hue-saturation values into 18 bins.

\subsubsection{Texton Features}
The idea of textons was first introduced by Olshausen and Field as fundamental micro-structures in generic natural images (\cite{Javidi12_Journal} TODO: Fix Citation). We utilize the version first introduced by Leung and Malik in  (\cite{Javidi12_Journal} TODO: Fix Citation). Here we convolve the image patch with a series of gabor filters at six evenly spaced orientations at two different pyramid scales.

Once the patch is the image patch is convolved with this series of gabor filters, we take each individual filter response of each image patch and cluster them in an unsupervised manner using k-means clustering. (In our implementation we use k=24 for computational efficiency reasons). Once the responses from the filters are clustered into visual words, we quantize the gabor filter responses corresponding to the computed

visual words using a nearest neighbour approach. This histogram of quantized responses now serves as a texton feature.

\subsubsection{Entropy Map}
To quantify the entropy of the local image patch we quantize the colors of the image into RGB histograms. These histograms are normalized and their frequency is computed by summing the bins in each individual channel. The entropy of the image is then computed by summing the shannon entropy of each bin in each channel.

This is computed as shown below:

\[
Entropy = \sum_jp_j*log(p_j)\\
\]

where $p_j$ is given by (Bin Value)/(Frequency of Channel). Hence this gives us three values for entropy, one for each color channel.

\subsubsection{Push Template}
Here we introduce the notion of a push template. A push template is a region in the point cloud which is amenable to pushing by a manipulator. To define such a template we extract holes in images, which are recovered by projecting euclidean clusters on some nominal plane and looking for voids on this plane. Once these voids are detected,

we extract the largest pointcloud surrounding such a void in an image by looking at the total number of points inside an oriented bounding box. The orientations of the bounding box are determined by uniformly discretizing the space of all yaws. The size of the bounding box is constrained by the radius of the size of the gipper of the manipulator in use.

Of all the tested orientations the bounding box with the maximum number of point is selected. We then compute quantized local normals ( Fast Point Feature Histograms \cite{Javidi12_Journal} TODO: Fix Citation) for the pointcloud inside this bounding box and take the mean of the feature computed over all points.

\subsubsection{Grasp Templates}
As a final feature we compute grasp templates which are local heightmaps computed for a given sensor viewpoint and gripper pose. These features were first introduced by Herzhog et al in (\cite{Javidi12_Journal} TODO: Fix Citation). The reader is encouraged to read the paper mentioned to learn about the computation of this specific feature.

Once these features are computed we concatenate these features in to a 112 dimensional feature vector to describe the current scene graph $mathcal{G}$. Once we record this feature $\mathcal{F}$ for a given scene graph, then we execute one of the actions in our action set $\mathcal{A}$ and label the feature as 1-0 based on the observed reward.

\subsection{Max Entropy Learning}
After number of executions of each action we take the labeled features $\mathcal{F}_l$ and run them through a Max-Entropy Learner to learn weights on these features. We learn a set of weights $w_i$ corresponding to each action ${a_1,...a_i}\in \mathcal{A}$. We use L-1 logistic regression as our max entropy learner.

The objective function we try to optimize in our learner is given by:
\[
J = \sum_{i=1}^m-log(\frac{1}{1+\exp^{w^Tx_iy_i}}) + \lambda||w||_1\\
\]

This objective function is optimized by the limited memory version of the BFGS algorithm namely, L-BFGS. At test time the labels of a given feature can be determined by 

\[
y_{test} = sgn(w^Tx_{test})\\
\]

We use L-1 regularized logistic regression as opposed to L-2 regularized logistic regression as it performs better when the dimensionality of the features exceeds that of the number of available training samples. (TODO: Cite that paper that Mrinal cites in his work). We use these learned weigths in conjunction with the features observed online by sampling from a gibbs distribution as suggested by (\cite{Javidi12_Journal} TODO: Fix Citation, Cite Peters and Bagnell)

\[
\pi(a|s) \sim (\frac{\exp^{\phi(s,a)^T\theta}}{ \sum_b\exp^{\phi(s,b)^T\theta}})\\
\]

The action selection and the online adaptation of the weights is discussed in more detail in Section \ref{sec:hyp_test}.


% We would like to use a Bayesian framework to maintain probabilities for the object hypotheses. This requires statistics about the operation of the sensor for different object classes, orientations, and viewpoints. Instead of using the segmented pointcloud $\mathcal{Q}_t$ as the observation of the sensor, we take the output of the vocabulary tree. As a result, we deal with the space of possible vocabulary tree outputs rather than the space of all possible pointclouds. Moreover, this includes the operation of the vision algorithm in the sensor statistics. 
% 
% Given a query pointcloud $\mathcal{Q}_t$ suppose that the vocabulary tree returns template $\mathcal{P}_{g,l}$ as the top match. Assume that the models in the training database are indexed so that those from $B_2$ have a lower $l$ index than those from $B_1 \setminus B_2$. We take the linear index of the closest match $\mathcal{P}_{g,l}$ as the observation if the match is an object of interest. Otherwise, we record only the model index $l$, ignoring the viewpoint $g$:
% \[
% Z_t = \begin{cases}
% (l-1)G+g, & \text{if } l \leq L_2\\
% L_2G+(l-L_2), & \text{if } l > L_2, \forall g \in \{1,\ldots,G\}.
% \end{cases}
% \]
% This makes the observation space one dimensional. Given a sensor pose $x \in V(\rho)$ and an object hypothesis $H_i$, we need to approximate the data likelihood of $Z_t$:
% \[
% h_i^x(z) := \mathbb{P}(Z_t = z \mid x, H_i)
% \]
% The function $h$ is called the \textit{observation model} of the static detector. It can be obtained off-line since it only depends on the characteristics of the sensor and the vision algorithm. Ideally, the sensing and detection processes should be abstracted to obtain a closed-form representation of $h$ but this is a daunting task for a depth sensor and a vocabulary tree. Instead, we learn a histogram approximating $h$ using the training dataset $B_1$.
% 
% The viewsphere is discretized into a set of viewpoints $\mathcal{X}(\rho) \subset V(\rho)$, which will be used in the planning phase. It need \textit{not} be the same as the set $V_G(\rho)$ used to train the vocabulary tree. We generated $50$ random environments from the models in $B_1$ for each of the $7$ hypotheses and used a simulated depth sensor to obtain scores from the vocabulary tree for a set $\mathcal{X}(\rho)$ of $42$ viewpoints.
